# MathAI v2

AI-powered mathematical assistant with grounded explanations, real-time streaming, and full observability.

## âœ¨ Features

- **Accurate Math**: SymPy ensures mathematically correct results (no hallucinations)
- **Grounded Explanations**: LLM explains but doesn't compute
- **Real-time Streaming**: See answers instantly, explanations stream in
- **Smart Caching**: Repeated queries return instantly
- **Parallel Execution**: Compute and RAG run simultaneously
- **Full Observability**: LangFuse tracing for debugging and evaluation
- **Beautiful Frontend**: Modern Next.js UI with LaTeX rendering

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         User Query                               â”‚
â”‚          "differentiate sin(x^2) with respect to x"              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Cache Check                                 â”‚
â”‚           If cached â†’ Return instantly (0ms)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ (cache miss)
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    1. LLM Router                                 â”‚
â”‚        Classifies â†’ differentiate, integrate, simplify, solve    â”‚
â”‚        Extracts â†’ expression, variable, assumptions              â”‚
â”‚        Model: gpt-4o-mini (fast, lightweight)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚      PARALLEL             â”‚
              â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. SymPy Compute    â”‚    â”‚   3. RAG Retrieval   â”‚
â”‚  Authoritative math  â”‚    â”‚  Semantic search     â”‚
â”‚  Result: 2*x*cos(xÂ²) â”‚    â”‚  Knowledge chunks    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                           â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                4. LLM Explanation (Streaming)                    â”‚
â”‚         Explains the SymPy result (no new computation)           â”‚
â”‚         Grounded by: query + routing + result + RAG chunks       â”‚
â”‚         Model: gpt-4o-mini                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Response + Cache                              â”‚
â”‚         âœ“ Answer: 2*x*cos(xÂ²) (from SymPy)                       â”‚
â”‚         âœ“ Explanation: Streamed step-by-step                    â”‚
â”‚         âœ“ Cached for instant repeat queries                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸš€ Quick Start

### Prerequisites

- Python 3.10+
- Node.js 18+
- OpenAI API key
- (Optional) LangFuse account for observability

### 1. Backend Setup

```bash
cd MathAI

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp config.example.txt .env
# Edit .env with your API keys
```

### 2. Frontend Setup

```bash
cd mathai-frontend

# Install dependencies
npm install --legacy-peer-deps

# Configure environment
echo "NEXT_PUBLIC_API_URL=http://localhost:8000" > .env.local
```

### 3. Run the Application

**Terminal 1 - Backend:**
```bash
cd MathAI
source venv/bin/activate
python app.py
```

**Terminal 2 - Frontend:**
```bash
cd mathai-frontend
npm run dev
```

**Access:**
- Frontend: http://localhost:3000
- Backend API: http://localhost:8000

## ðŸ“¡ API Endpoints

### POST `/solve`

Standard endpoint for math queries.

```bash
curl -X POST http://localhost:8000/solve \
  -H "Content-Type: application/json" \
  -d '{"query": "integrate x^2 * e^x dx"}'
```

**Response:**
```json
{
  "success": true,
  "query": "integrate x^2 * e^x dx",
  "operation": "integrate",
  "answer": "(x**2 - 2*x + 2)*exp(x)",
  "latex_answer": "\\left(x^{2} - 2 x + 2\\right) e^{x}",
  "explanation": "Step-by-step explanation...",
  "assumptions": [],
  "citations": ["integrate_0", "integrate_1"]
}
```

### POST `/solve/stream`

Streaming endpoint for real-time responses (used by frontend).

Returns Server-Sent Events:
1. `{"type": "answer", "data": {...}}` - Answer arrives first
2. `{"type": "explanation", "data": "token"}` - Explanation streams in
3. `{"type": "done"}` - Complete

### GET `/health`

Health check with LangFuse status.

```json
{
  "status": "healthy",
  "version": "2.0.0",
  "langfuse_enabled": true
}
```

## âš¡ Performance Optimizations

| Optimization | Impact |
|-------------|--------|
| **Smart Caching** | Repeated queries: **0ms** |
| **Parallel Execution** | Compute + RAG run together: saves **~1s** |
| **Streaming** | Answer appears in **~2s**, explanation streams |
| **gpt-4o-mini** | Faster and cheaper than gpt-4o |

### Typical Latency

| Query Type | Time |
|------------|------|
| First query (cold) | ~5-8s |
| Cached query | **<10ms** |
| With streaming | Answer in ~2s, full response ~8s |

## ðŸ“Š LangFuse Observability

When configured, MathAI traces every request:

| Span | Metrics Captured |
|------|-----------------|
| **math_router** | Latency, tokens, confidence |
| **sympy_compute** | Execution time, success/failure |
| **rag_retrieval** | Chunk IDs, relevance scores |
| **llm_explanation** | Token usage, cost, latency |

### Setup LangFuse

1. Create account at [cloud.langfuse.com](https://cloud.langfuse.com)
2. Add to `.env`:
   ```
   LANGFUSE_ENABLED=true
   LANGFUSE_PUBLIC_KEY=pk-lf-xxxxx
   LANGFUSE_SECRET_KEY=sk-lf-xxxxx
   LANGFUSE_HOST=https://cloud.langfuse.com
   ```

## ðŸ“ Project Structure

```
MathAI/
â”œâ”€â”€ MathAI/                    # Backend
â”‚   â”œâ”€â”€ app.py                 # FastAPI application
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ models.py          # Pydantic data models
â”‚   â”‚   â”œâ”€â”€ router.py          # LLM-based query router
â”‚   â”‚   â”œâ”€â”€ compute.py         # SymPy computation engine
â”‚   â”‚   â”œâ”€â”€ rag.py             # RAG retrieval system
â”‚   â”‚   â”œâ”€â”€ explainer.py       # LLM explanation generator
â”‚   â”‚   â”œâ”€â”€ pipeline.py        # Main orchestrator (caching, parallel)
â”‚   â”‚   â””â”€â”€ instrumentation.py # LangFuse integration
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ vectordb/          # ChromaDB storage
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env                   # Environment variables
â”‚
â””â”€â”€ mathai-frontend/           # Frontend
    â”œâ”€â”€ app/
    â”‚   â”œâ”€â”€ page.tsx           # Main UI with streaming
    â”‚   â”œâ”€â”€ layout.tsx
    â”‚   â””â”€â”€ globals.css
    â”œâ”€â”€ package.json
    â””â”€â”€ .env.local             # Frontend config
```

## ðŸ”§ Supported Operations

| Operation | Example Queries |
|-----------|-----------------|
| **Differentiate** | "find the derivative of ln(x)", "d/dx sin(x^2)" |
| **Integrate** | "integrate e^x * cos(x)", "find antiderivative of 1/x" |
| **Simplify** | "simplify (x^2-1)/(x-1)", "expand (a+b)^3" |
| **Solve** | "solve x^2 + 2x - 3 = 0", "find roots of x^3 - x" |

## ðŸ§ª Development

### Running Tests

```bash
cd MathAI
pytest tests/ -v
```

### Debug Mode

Set `MATHAI_ENV=development` to enable debug endpoints:

- `POST /debug/route` - See routing decision
- `POST /debug/compute` - See compute result

## ðŸ“ Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `OPENAI_API_KEY` | Yes | OpenAI API key |
| `ROUTER_MODEL` | No | Router model (default: gpt-4o-mini) |
| `EXPLAINER_MODEL` | No | Explainer model (default: gpt-4o-mini) |
| `LANGFUSE_ENABLED` | No | Enable LangFuse (default: true) |
| `LANGFUSE_PUBLIC_KEY` | No | LangFuse public key |
| `LANGFUSE_SECRET_KEY` | No | LangFuse secret key |
| `LANGFUSE_HOST` | No | LangFuse host URL |
| `PORT` | No | Server port (default: 8000) |

